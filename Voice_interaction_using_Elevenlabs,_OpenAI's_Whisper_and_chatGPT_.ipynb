{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddrbcn/voicegpt/blob/main/Voice_interaction_using_Elevenlabs%2C_OpenAI's_Whisper_and_chatGPT_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak5iOoGhGn5J"
      },
      "source": [
        "# ü§ñ üîä **Use ChatGPT with your voice!** using OpenAI's Whisper and the OpenAI and ElevenLabs APIs. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
        "\n",
        "This notebook will help you use the OpenAI and ElevenLabs APIs to generate text using artificial intelligence. Follow the instructions to input your API keys and select a voice. You will only need to run the following code cells step by step.\n",
        "\n",
        "üìΩÔ∏è *The notebook is based on the [original notebook](https://colab.research.google.com/drive/1qY-6J4UpKZ0tOmhNmh0Ci6MSiCo6xBTP?usp=sharing) created by [DotCSV](https://www.youtube.com/channel/UCy5znSnfMsDwaLlROnZ7Qbg) and GPT-4. (in Spanish.)*\n",
        "\n",
        "üê¶ *Don't forget to follow it on [Twitter](https://twitter.com/dotCSV) to stay updated on their latest posts and projects.*\n",
        "\n",
        "\n",
        "**I also used content from the [notebook:](https://colab.research.google.com/github/fastforwardlabs/whisper-openai/blob/master/WhisperDemo.ipynb#scrollTo=v5hvo8QWN-a9)**\n",
        "# Make your own recordings and transcriptions with OpenAI's Whisper!\n",
        "_a fun diversion brought to you by [Melanie](https://www.linkedin.com/in/melanierbeck/), ML Research Manager at Cloudera Fast Forward Labs_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giEwWFesIOT3"
      },
      "source": [
        "## **Step 1:** Access and register to the OpenAI and ElevenLabs APIs\n",
        "\n",
        "\n",
        "> <p>‚úèÔ∏è <b>OpenAI Website</b> <i>(Text Generation)</i>\n",
        "<br>\n",
        "<a href=\"https://platform.openai.com/account/api\">https://platform.openai.com/account/api-keys</a>\n",
        "\n",
        "<p>üîä <b>ElevenLabs Website</b> <i>(Text-to-Speech Synthesis)</i>\n",
        "<br>\n",
        "<a href=\"https://beta.elevenlabs.io/\">https://beta.elevenlabs.io/</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJzVtF1EJ7Rw"
      },
      "source": [
        "## **STEP 2:** Configure your API access.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs and imports\n",
        "The commands below will install the Python packages needed to use GPT model, Elevenlabs voices, record audio snippets and use Whisper models for speech-to-text transcription."
      ],
      "metadata": {
        "id": "Kl5OJmozZpKr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1vgGNPoKz-K"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai\n",
        "!pip install -q elevenlabs\n",
        "\n",
        "import os\n",
        "import openai\n",
        "import tempfile\n",
        "import requests\n",
        "from IPython.display import Audio, clear_output\n",
        "from elevenlabs import generate, play, set_api_key, voices, Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install sounddevice wavio\n",
        "! pip install ipywebrtc notebook"
      ],
      "metadata": {
        "id": "wMdjl-srS5p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the key APIs of both tools and add them to the following form.\n"
      ],
      "metadata": {
        "id": "-n8CD3Ik-OBe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "725pmrn35M0D"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "openai_api_key     = \"\" #@param {type:\"string\"}\n",
        "eleven_api_key = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# Configure GPT-4 and Text-to-speech API keys\n",
        "openai.api_key = openai_api_key\n",
        "set_api_key(eleven_api_key)\n",
        "\n",
        "voice_list = voices()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need the following in order to record audio from this notebook and process the resulting files."
      ],
      "metadata": {
        "id": "zqO_3x0taMqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install ffmpeg\n",
        "!apt-get install libportaudio2"
      ],
      "metadata": {
        "id": "RLJBw9MuUHmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import torchaudio\n",
        "\n",
        "from ipywebrtc import AudioRecorder, CameraStream\n",
        "from IPython.display import Audio, display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "J_Njp109Ugdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdFOTFBPCLRp"
      },
      "source": [
        "## **STEP 3:** Select the voice to use.\n",
        "\n",
        "Run the code in the next cell and choose the voice you want to interact with. If you have cloned your voice on the *ElevenLabs* website, you will see the voice you created included in the list.\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEsm4oHHMts5"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import ipywidgets as widgets\n",
        "\n",
        "voice_labels = [voice.category + \" voice: \" + voice.name for voice in voice_list]\n",
        "\n",
        "voice_id_dropdown = widgets.Dropdown(\n",
        "    options=voice_labels,\n",
        "    value=voice_labels[0],\n",
        "    description=\"Selecciona una voz:\",\n",
        ")\n",
        "\n",
        "display(voice_id_dropdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 4:** Select language options.\n",
        "\n",
        "**Whisper is capable of performing transcriptions for many languages** (though it performs better for some languages and worse for others.)\n",
        "\n",
        "Whisper is also capable of detecting the input language. However, **to be on the safe side, we can explicitly tell Whisper which language to expect**."
      ],
      "metadata": {
        "id": "WoZDqlE3cBra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_options = whisper.tokenizer.TO_LANGUAGE_CODE\n",
        "language_list = list(language_options.keys())"
      ],
      "metadata": {
        "id": "rAzB4nueZtwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang_dropdown = widgets.Dropdown(options=language_list, value='english')\n",
        "output = widgets.Output()\n",
        "display(lang_dropdown)"
      ],
      "metadata": {
        "id": "cdiqb1eyZu4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whisper is also capable of several tasks, including English-only transcription, Any-to-English translation, and non-English transcription.\n",
        "\n",
        "Below you can select either \"transcription\" (which will yield text in the same language as the input language)."
      ],
      "metadata": {
        "id": "2JZ_3PPWdX6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task_dropdown = widgets.Dropdown(options=['transcribe', 'translate'], value='transcribe')\n",
        "output = widgets.Output()\n",
        "display(task_dropdown)"
      ],
      "metadata": {
        "id": "bT5EMBNBaFUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 5:** Load Whisper model\n",
        "\n",
        "Whisper comes in five model sizes, four of which also have an optimized English-only version. This notebook loads \"base\"-sized models (bigger than \"tiny\" but smaller than the others), which **require about 1GB of RAM**.\n",
        "\n",
        "If you selected English above, the cell below will load the optimized English-only version. Otherwise, it will load the multilingual model."
      ],
      "metadata": {
        "id": "P4DvT6EXdyfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if lang_dropdown.value == \"english\":\n",
        "  model = whisper.load_model(\"base.en\")\n",
        "else:\n",
        "  model = whisper.load_model(\"base\")\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ],
      "metadata": {
        "id": "DT7ccbdrc96s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "options = whisper.DecodingOptions(language=lang_dropdown.value, task=task_dropdown.value, without_timestamps=True, fp16=False)\n",
        "options"
      ],
      "metadata": {
        "id": "0Mni81ODdOMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to enable some Colab widgets so that we can make an audio recording."
      ],
      "metadata": {
        "id": "hSAjkFROfVUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "2jS643WvguA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDrGe0JoDB7h"
      },
      "source": [
        "## **STEP 6:** Set Up and Interact with ChatGPT\n",
        "\n",
        "You can choose below **which version of ChatGPT you want to talk to**. Please note that the GPT-4-based version comes at a higher cost than the GPT-3.5 model. Refer to the pricing table at the following link before using it.\n",
        "\n",
        "üëâ [**ChatGPT Pricing Table**](https://openai.com/pricing)\n",
        "\n",
        "You can also **customize the behavior of the ChatGPT model** by modifying the system message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtHp8FwaDt-V"
      },
      "outputs": [],
      "source": [
        "#@title Configuraci√≥n de ChatGPT.\n",
        "chatgpt_model = \"gpt-3.5-turbo\" #@param [\"gpt-3.5-turbo\", \"gpt-4\"]\n",
        "\n",
        "chatgpt_system = \"You are a helpful assistant on a conversation. Answer should be not too long. Be kind and nice.\" #@param {type:\"string\"}\n",
        "\n",
        "# Encuentra el √≠ndice de la opci√≥n seleccionada\n",
        "selected_voice_index = voice_labels.index(voice_id_dropdown.value)\n",
        "selected_voice_id    = voice_list[selected_voice_index].voice_id\n",
        "\n",
        "# Function to get GPT-4 response\n",
        "def get_gpt4_response(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=chatgpt_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": chatgpt_system},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Main function to interact with GPT-4\n",
        "def interact_with_gpt4(prompt):\n",
        "    response_text = get_gpt4_response(prompt)\n",
        "\n",
        "    import requests\n",
        "\n",
        "    CHUNK_SIZE = 1024\n",
        "    url = \"https://api.elevenlabs.io/v1/text-to-speech/\" + selected_voice_id\n",
        "\n",
        "    headers = {\n",
        "      \"Accept\": \"audio/mpeg\",\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"xi-api-key\": eleven_api_key\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "      \"text\": response_text,\n",
        "      \"model_id\" : \"eleven_multilingual_v2\",\n",
        "      \"voice_settings\": {\n",
        "         \"stability\": 0.5,\n",
        "         \"similarity_boost\": 0.75,\n",
        "         \"style\": 0,\n",
        "          #\"use_speaker_boost\": true\n",
        "      }\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=data, headers=headers)\n",
        "\n",
        "    # Save audio data to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "        f.flush()\n",
        "        temp_filename = f.name\n",
        "\n",
        "    return temp_filename\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time to record the prompt!\n",
        "\n",
        "Press the circle button and start speaking. **It may not look it, but the widget will be capturing sound**. Click the circle button again when you are finished. The widget will immediately begin to play back what it captured."
      ],
      "metadata": {
        "id": "4ikvRgONh8O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "camera = CameraStream(constraints={'audio': True,'video':False})\n",
        "recorder = AudioRecorder(stream=camera)\n",
        "recorder"
      ],
      "metadata": {
        "id": "CT31o-LGfHEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sending the prompt to GPT"
      ],
      "metadata": {
        "id": "LYWNLffti5pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clear_output(wait=True)\n",
        "\n",
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav my_recording.wav -y -hide_banner -loglevel panic\n",
        "\n",
        "audio = whisper.load_audio(\"my_recording.wav\")\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "result = model.decode(mel, options)\n",
        "prompt = result.text\n",
        "print(prompt)\n",
        "\n",
        "audio_file = interact_with_gpt4(prompt)\n",
        "play(audio_file, notebook=True)"
      ],
      "metadata": {
        "id": "GdiHBzFLiFsh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}